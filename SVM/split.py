"""
Data Preprocessing and Train/Test Split Generator for Multi-Factor Authentication

This script processes authentication data by:
1. Loading and normalizing feature data
2. Generating synthetic attacker data to balance the dataset
3. Creating balanced train/test splits for machine learning models

The script handles both legitimate user data (label=0) and synthetic attacker data (label=1)
to create a balanced dataset suitable for binary classification tasks.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import os
from typing import List, Tuple, Dict
import sys

# Directory where output train.csv and test.csv files will be saved
OUTPUT_DIR = "output"

# Boolean feature columns that need conversion to integers (0/1)
BOOLEAN_COLS: List[str] = ['is_rooted', 'vpn_tor_usage']

# Numeric feature columns used for machine learning model training
# These represent various authentication factors and user behavior patterns
NUMERIC_COLS: List[str] = [
    'region_tz_code', 'os_code', 'device_type_code', 'manufacturer_code',
    'gps_latitude', 'gps_longitude', 'location_conf_radius', 'location_visit_count',
    'shift_profile_code', 'session_start_epoch', 'session_duration_mins',
    'time_since_last_login_mins', 'day_type_code', 'ip_address_as_int',
    'ip_reputation_code', 'typing_speed_cpm', 'click_pattern_code',
    'role_code', 'scope_code', 'failed_login_attempts', 'historic_risk_score', 'system_mode_code'
]

def _bool_to_int(x) -> int:
    """
    Convert various boolean representations to integer values (0 or 1).
    
    This helper function handles different formats of boolean data that might
    appear in the dataset, including Python booleans, string representations,
    and numeric values.
    
    Args:
        x: Input value that could be bool, str, int, or other type
        
    Returns:
        int: 0 for false/negative values, 1 for true/positive values
    """
    if isinstance(x, bool): return int(x)
    if isinstance(x, str): return 1 if x.lower() == "true" else 0
    try:
        return int(x)
    except (ValueError, TypeError):
        return 0

def normalize_features(df: pd.DataFrame, features_to_scale: List[str]) -> Tuple[pd.DataFrame, MinMaxScaler]:
    """
    Normalize feature data using MinMax scaling to range [0,1].
    
    This function prepares the data for machine learning by:
    1. Converting boolean columns to integers (0/1)
    2. Applying MinMax scaling to normalize all features to [0,1] range
    
    MinMax scaling is important for SVM and other distance-based algorithms
    to ensure all features contribute equally regardless of their original scale.
    
    Args:
        df (pd.DataFrame): Input dataframe with raw features
        features_to_scale (List[str]): List of column names to apply scaling to
        
    Returns:
        Tuple[pd.DataFrame, MinMaxScaler]: Normalized dataframe and fitted scaler object
    """
    df_copy = df.copy()
    
    # Convert boolean columns to integer representation (0/1)
    for col in BOOLEAN_COLS:
        if col in df_copy.columns:
            df_copy[col] = df_copy[col].apply(_bool_to_int)
    
    # Only scale columns that actually exist in the dataframe
    cols_to_scale_existing = [col for col in features_to_scale if col in df_copy.columns]
    
    # Fit MinMax scaler and transform the data to [0,1] range
    scaler = MinMaxScaler()
    df_scaled_values = scaler.fit_transform(df_copy[cols_to_scale_existing])
    df_scaled = pd.DataFrame(df_scaled_values, columns=cols_to_scale_existing, index=df_copy.index)
    
    return df_scaled, scaler

def generate_synthetic_data(base_df: pd.DataFrame, count: int) -> pd.DataFrame:
    """
    Generate synthetic attacker data by adding noise to existing legitimate user data.
    
    This function creates synthetic "attacker" samples to balance the dataset, which is
    crucial for training effective binary classifiers. The synthetic data is generated by:
    1. Randomly sampling from existing legitimate user data
    2. Adding small amounts of Gaussian noise to create variations
    3. Clipping values to maintain [0,1] range after normalization
    
    Args:
        base_df (pd.DataFrame): Normalized legitimate user feature data
        count (int): Number of synthetic attacker samples to generate
        
    Returns:
        pd.DataFrame: Synthetic attacker data with the same feature structure
    """
    # Sample with replacement from legitimate user data as base for attackers
    synthetic_samples = base_df.sample(n=count, random_state=42, replace=True).copy()
    
    # Add small amount of Gaussian noise to create variations
    # Standard deviation of 0.03 provides subtle but meaningful differences
    noise = np.random.normal(0, 0.03, synthetic_samples.shape)
    synthetic_samples += noise
    
    # Ensure all values remain in [0,1] range after adding noise
    synthetic_samples = np.clip(synthetic_samples, 0, 1)
    
    return pd.DataFrame(synthetic_samples, columns=base_df.columns)

def main():
    """
    Main function that orchestrates the entire data preprocessing pipeline.
    
    Process flow:
    1. Parse command line arguments for attacker count and data path
    2. Load and validate input data
    3. Normalize features and prepare legitimate user data (label=0)
    4. Generate synthetic attacker data (label=1)
    5. Combine and shuffle the balanced dataset
    6. Split into train/test sets with stratification
    7. Save final datasets to CSV files
    
    Command line usage:
        python split.py <attacker_count> [data_path]
    """
    # Create output directory if it doesn't exist
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Parse command line arguments
    ATTACKER_COUNT = sys.argv[1]
    data_path = sys.argv[2]
    
    # Validate attacker count parameter
    if not ATTACKER_COUNT.isdigit() or int(ATTACKER_COUNT) <= 0:
        print("python split.py <attacker_count>")
        exit(1)
    
    # Use default data path if provided path doesn't exist
    if not os.path.exists(data_path):
        print("Default data will be used")
        data_path = "../GAN/output_with_trust_scores.csv"

    INPUT_CSV_PATH = data_path

    # Load input data with error handling
    try:
        df = pd.read_csv(INPUT_CSV_PATH)
    except FileNotFoundError:
        print(f"Error: Input file not found at '{INPUT_CSV_PATH}'. Please check the path.")
        return
    
    # Define machine learning feature columns (exclude 'scope_code' if present)
    ml_feature_cols = [col for col in NUMERIC_COLS + BOOLEAN_COLS if col != 'scope_code' and col in df.columns]
    
    # Validate that all expected features are present in the data
    missing_cols_in_df = [col for col in ml_feature_cols if col not in df.columns]
    if missing_cols_in_df:
        print(f"Error: Missing expected feature columns in '{INPUT_CSV_PATH}': {missing_cols_in_df}")
        return
    
    # Extract and normalize legitimate user features
    df_features_original = df[ml_feature_cols].copy()
    df_scaled_features_original, _ = normalize_features(df_features_original, ml_feature_cols)
    
    # Label legitimate users as 0 (non-attackers)
    df_scaled_features_original['label'] = 0
    
    # Generate synthetic attacker data and label as 1 (attackers)
    synthetic_features = generate_synthetic_data(df_scaled_features_original.drop(columns=['label']), int(ATTACKER_COUNT))
    synthetic_features['label'] = 1
    
    # Combine legitimate and synthetic data into one balanced dataset
    full_df_with_labels = pd.concat([df_scaled_features_original, synthetic_features], ignore_index=True)
    
    # Shuffle the combined dataset to ensure random distribution
    full_df_with_labels = full_df_with_labels.sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Separate features (X) and labels (y) for train/test splitting
    X = full_df_with_labels.drop(columns=['label'])
    y = full_df_with_labels['label']
    
    # Create stratified train/test split (70/30) to maintain class balance
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.3,        # 30% for testing
        random_state=42,      # For reproducible results
        stratify=y            # Maintain class balance in both sets
    )
    
    # Recombine features and labels for final datasets
    train_df_final = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)
    test_df_final = pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)
    
    # Save train and test datasets to CSV files
    train_df_final.to_csv(os.path.join(OUTPUT_DIR, "train.csv"), index=False, header=True)
    test_df_final.to_csv(os.path.join(OUTPUT_DIR, "test.csv"), index=False, header=True)
    
    # Print summary statistics about the generated datasets
    print(f"[+] Successfully wrote train.csv and test.csv to '{OUTPUT_DIR}'")
    print(f"Total blended samples (original + synthetic): {len(full_df_with_labels)}")
    print(f"Train set size: {len(train_df_final)} (Attackers: {train_df_final['label'].sum()})")
    print(f"Test set size: {len(test_df_final)} (Attackers: {test_df_final['label'].sum()})")
    print("\nClass Distribution in Combined Data:")
    print(full_df_with_labels['label'].value_counts(normalize=True))

if __name__ == "__main__":
    main()
